\documentclass[11pt, a4paper]{report}
\clubpenalty=10000
\widowpenalty=10000
\usepackage{fullpage}
\usepackage{epsfig}
\usepackage{setspace}
\usepackage{latexsym}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage[usenames]{color}
\usepackage{verbatim}

\newcommand\todo[1]{\textcolor{Red}{\textbf{#1}}}
\newcommand\pOlost{\textrm{$P_0$ is lost}}
\newcommand\QED{\hfill\ensuremath{\Box}}
\let\showproof1


\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\title{Streaming of High-Resolution Progressive Meshes Over Network}
\author{Wei CHENG\\
National University of Singapore\\
SuperVisor\\
Wei Tsang OOI}
\begin{document}
\maketitle
\doublespacing
\begin{abstract}
    \begin{comment}
    High-resolution 3D meshes are increasingly available in networked
    applications, such as digital museum, online game, and virtual reality.
    The amount of data constituting a high-resolution 3D mesh can be
    huge, leading to a long downloading time. 
    To reduce the waiting time of users, 
    a common technique for remote viewing is progressive streaming,
    which allows a low-resolution version of the mesh to be transmitted
    and rendered with low latency. Then the quality of the transmitted 
    mesh can be incrementally improved when the refinement information
    is continuously being transmitted.

    Progressive mesh is commonly used to support progressive
    streaming. A progressive mesh comprises a \emph{base mesh} and a series
    of refinements. The base mesh is obtained by simplifying the original mesh
    with a series of \emph{edge collapses}.
    With the \emph{vertex splits}, the inverses of these edge
    collapses, the original mesh can be reconstructed from the base mesh.
    Therefore, progressive streaming can be implemented by sending the vertex
    splits as refinements after sending the base mesh.

    Although streaming of videos is extensively studied, 
    streaming of high-resolution progressive meshes is considerably different.
    Frames of a video are usually sent following the time order,
    vertex splits of a progressive mesh, however, can be sent in various of orders.
    Some new research problems arise due to the flexibility in sending order of 
    vertex splits. In this thesis, four related topics are discussed. 
    %After decoding, each frame contributes the same to a video, but vertex splits
    %in a progressive mesh vary considerably in their contribution to the quality
    %of the reconstructed mesh. 
       
    First, 
    %the effect of dependency needs to be considered in choosing
    %the sending order of vertex splits.
    the progressive coding of meshes introduces dependencies among 
    the vertex splits, and the descendants cannot be decoded
    before their ancestors are all decoded. Therefore, 
    when a progressive mesh is transmitted over a lossy network,
    a packet loss will delay the decoding of the following
    vertex splits if they depend on this lost packet. 
    %These successfully received vertex splits cannot be 
    %decoded until the lost packet is successfully retransmitted. 
    Hence, the effect of dependency needs to
    be considered in choosing the sending order of vertex splits. 
    %To find the effect of dependency
    %is non-trivial as the packet loss happens randomly.
    In this thesis, an analytical model is proposed to 
    %quantify the effect of dependency on the quality curve.
    quantitatively analyzes the effects of dependency
    by modeling the distribution of decoding time of each vertex split
    as a function of mesh properties and network parameters.
    %The quality curve is determined by the decoding time of the vertex
    %splits and their contributions to the overall mesh quality.    
    %The decoding time of a vertex split depends on its receiving time and the 
    %receiving time of its all ancestors. The receiving time of a vertex split
    %in turn depends on its sending time and how many times it is 
    %transmitted. 
    %Hence, the distribution of decoding time of each vertex split
    %is modeled as a function of the mesh property and the network condition.
    %From this expression, %both the expected value and 
    %the distribution of the decoding time of each
    %vertex split can be efficiently computed before transmission. 
    Consequently, Different sending orders can be  
    efficiently evaluated without simulations, 
    and this model can help in developing a sending
    strategy to improve the quality curve 
    during transmission. % to improve the quality.
    
    Furthermore, %closed-form expressions are derived in two extreme cases,
    this model is applied to study two extreme cases of dependency in 
    progressive meshes and
    %providing insights to the importance of dependencies on the
    %decoded mesh quality. 
    The main insight found is that if each lost packet
    is immediately retransmitted upon the packet loss is detected, 
    the effect of dependencies on decoded mesh quality diminishes with time.
    %the dependencies only matters in the first several round trip times. 
    Therefore, the effect of dependency is only significant in the applications
    requiring high interactivity. 

    The accuracy of the analylitical model proposed in this thesis is validated
    under a variety of network conditions, including bursty losses, fluctuating RTT,
    and varying sending rate. The values predicted from our model match the measured
    value reasonably well in all cases except when losses are too bursty.
    
    To further improve the quality curve, the view-point of the 
    user can be considered in deciding the sending order.
    %Usually, users can only see a part of a mesh, so sending non-visible data
    %before visible data wastes bandwidth. Moreover, 
    Non-visible vertex splits do not need to be sent, and 
    the visual contributions of visible vertex splits (how much they can improve the rendered image)
    also depend on the view-point of the user.
    Hence, choosing the sending order according to the viewpoint of the user, so called
    \emph{view-dependent streaming}, is a natural choice.
    
    In existing solutions to view-dependent streaming,  the
    sender decides the sending order. 
    %This sender-driven protocol has several
    %drawbacks. First, it is not scalable to many receivers as the sender has to
    %determine the visibility of vertices and sort the visible vertex splits based on their
    %visual contributions for each receiver. 
    %Second, 
    The sender needs to maintain
    the rendering state of each receiver to avoid sending duplicate data. 
    Due to the stateful design %and high computational requirements,
    the sender-driven approach cannot be easily extended to support 
    many receivers with caching proxy and peer-to-peer system,
    two common solutions to scalability. 

    In this thesis, a receiver-driven protocol is proposed to 
    improve the scalability. In this protocol, the receiver decides the sending order
    and explicitly requests the vertex splits, 
    while the sender simply sends the data requested.
    The sending order is computed at the receiver  by estimating the visibility
    and visual contributions of the refinements, even before receiving them,
    with the help of GPU.

    Because the visibility determination and state maintenance are all done by the receivers, 
    the sender in our receiver-driven protocol is stateless and free of complex computation.
    Furthermore, caching proxy and P2P streaming systems can be applied to improve
    the scalability without adding more servers.  
    
    The receiver-driven protocol significantly reduces the computing cost at the sender,
    but the bandwidth can remain the bottleneck if each receiver receives data
    from the same sender. 
    Based on the receiver-driven protocol, P2P techniques are applied to mesh
    streaming in this thesis. In the implementation of P2P mesh streaming system,
    two issues are considered: how to partition a progressive mesh into chunks and 
    how to lookup the provider of a chunk. For the latter issus, we investigated 
    into two solutions, which trade off server overhead and response time. The first
    uses a simple centralized lookup service, while the second organizes peers into
    groups according to the hierarchical structure of the progressive mesh to 
    take advantage of access pattern. Simulation results show that our 
    proposed systems are robust under high churn rate, reduce the server overhead
    by more than $90\%$, keep control overhead below $10\%s$, and achieve
    low average response time.
\end{comment}
\end{abstract}
\chapter{Introduction}
\label{c:intro}
    Advances in 3D scanning technology and 3D modeling
    techniques have enabled creations of huge,
    high-resolution, 3D models.  
    For instance, software such as
    ZBrush is enabling easy creation of complex digital sculpture.
    The Stanford's Digital Michelangelo Project
    \cite{levoy00digital} digitized statues made by Michelangelo
    and provided a software, ScanView \cite{koller04protected},
    to allow users to remotely view the 3D version of the statues.
    Similar projects include the Digital Sculpture Project \cite{deroos2004dsp}
    and the project to digitize Rodin's sculptures \cite{miyazaki2006dab}. 

    Traditional, networked applications such as 3D games 
    store all data of 3D models in usersâ€™ local disks, 
    but now some networked applications store the data on the server side 
    and transmit them from the server to users when they are needed. 
    The latter method has several advantages. 
    First, users can immediately start the application without buying a CD
    or downloading large amount of data. 
    Second, because the 3D data are stored in the server and shared by all the users, 
    users can create and modify objects and others can see the new objects immediately. 
    For example, many objects in Second Life are user-created, 
    and this feature is believed to be essential to success of Second Life.Â 

    High resolution 3D models may take long time to download completely
    for display at the client. 
    For example, the Stanford model of the David statue, with 28 million vertices and
    56 million triangles, is still 70 MB in size with
    state-of-the-art compression \cite{alliez2001progressive} and needs around 10 minutes
    to download at 1 Mbps.
    
    To reduce the waiting time of the user, 
    the streaming technology can be used to transmit 3D models. 
    The 3D streaming technology enables users to render the partially received data as a preview, 
    and then improve the quality when more data are available. 
    Users can decide how many data to receive based on their requirement of quality
    and the rendering ability of their hardware.Â 

    Although audio and video streaming have been studied extensively, 
    3D streaming remains a new research area. 
    Video streaming and 3D streaming are significantly different
    due to the difference between videos and the representations of 3D models. 
    The main difference is how users consume the data.
    In video streaming, usually the data are consumed in time order. 
    Whereas in 3D streaming, especially when high-resolution 3D models are used,
    users may consume different subset of the data, and in various orders.
    
    This flexibility is important in streaming of high-resolution 3D models, 
    because streaming a huge 3D model in a fix order may waste time and bandwidth in 
    transmitting lots of data not useful to the user. 
    To ensure this flexibility in streaming system is an important consideration.
    Meanwhile, this flexibility introduces some new research questions, 
    which are the main topics of this thesis.
    Before introducing these new research questions,
    it is important to introduce some backgrounds and related works.
    \section{Backgrounds and Related Works}
    \subsection{Representation}
    \label{s:intro:representation}
    A 3D object is represented with a certain model.
    Currently the most popular representation of 3D objects is polygonal meshes, 
    especially the triangle meshes, because most graphic cards are optimized for triangle meshes. 

    A triangle mesh can be donated as a tuple $(K, V)$, where $K$ is a
    \emph{simplicial complex} including all the mesh simplices 
    (vertices, edges and triangles) and $V =\{v_{1}, v_{2}, ...,
    v_{n}\}$ is the set of geometry positions of vertices. Therefore,
    a mesh has both \emph{connectivity} information represented by $K$
    and \emph{geometry information} represented by $V$. In another
    word, $K$ includes all the topology information of all the
    elements in the mesh, and with $V$ it is embedded into $R^{3}$. If
    polygons are allowed in $K$ instead of only triangles, then this
    kind of meshes are called polygonal meshes.

    In a mesh, it is required that every
    vertex is an end of an edge and every edge should be incident to at
    least one face ((a) in figure \ref{mesh_non_mesh} is not a mesh).
    The number of edges incident to a vertex is called the
    \emph{valence} of this vertex, and the number of edges incident to
    a face is called the \emph{degree} of this face. An edge only
    incident to one face is called a \emph{boundary edge}; an edge
    shared by two faces is called an \emph{interior edge}; an edge
    shared by more than two faces is called a \emph{singular edge}. A
    \emph{manifold mesh} is a mesh without singular edges and every
    two faces share one edge or nothing (see figure \ref{mesh_non_mesh}).
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figure2.1.eps}
\caption[Manifold mesh, non-manifold mesh and non-mesh]{ (a) is not
a mesh since one edge is incident to no face. (b) is a manifold
mesh. (c) is a non-manifold mesh since there is a singular edge.
(d) is a non-manifold mesh since two faces share only a
vertex.\label{mesh_non_mesh}}\
\end{figure}

    
    Polygonal meshes can be used as an approximation of the surfaces
    of 3D objects. In this case, some properties of the surface such
    as color and normal are often associated to the mesh. These
    properties can be categorized into two types: discrete attributes
    and scalar attributes \label{property}. Discrete attributes, such
    as material identifier, are usually associated to the faces, while
    scalar attributes, such as color, normal, and texture coordinate,
    are often associated to vertices or corners (the tuple (vertex,
    face)). The latter one is used because in the case of
    discontinuity, a vertex can have more than one scalar value. For
    example, if two smooth surface intersect at a curve, then vertices
    in this curve will have two different normal values. Therefore,
    the normal value should be associated to the tuple (vertex face).
    Then, polygonal meshes can be represented by $(K, V, D, S)$ where
    $D$ is the set of discrete attributes associated to face $f \in
    K$ and $S$ is the set of scalar attributes associated to corners $(v, f)$ in $S$.
    
    \subsection{Coding and Compression}
    A mesh is encoded first before it can be stored or
    transmitted. Coding is to represent a mesh with a sequence of
    digits. Compression is always combined with coding to save the storage space
    or the bandwidth needed in transmission. 
    There are two kinds of coding:
    single-rate coding and progressive coding. 
    Single-rate coded mesh can only be decoded when all data are available 
    whereas progressive coded mesh can be decoded with only part of data are available. 
    We will briefly introduce single-rate coding methods, and then concentrate on
    progressive coding methods, because the latter are more suitable for progressive streaming.
    
    \subsubsection{Single-rate Coding} \label{single_rate}
    In a mesh, three kind of information needs to be coded:
    connectivity, geometry and property. 
    Since most property data are associated to vertices, 
    in many compress algorithms they are treated as same as geometry data or even
    ignored. We introduce connectivity coding first, and then introduce geometry coding briefly.
    
    A triangle mesh is often represented as an array of vertex coordinates
    (geometry information) and an array of triangles (connectivity information). 
    A triangle is represented by the indices of its three vertices.
    In this representation, every vertex is indexed by all the
    triangles to which it is incident. Repeated references introduce
    redundancy. Therefore, triangle strip method is introduced to
    reduce this redundancy. In a triangle strip, a triangle fan, or a
    generalized triangle strip (a mixture of triangle strips and
    triangle fans) (see Figure \ref{strip}), 
    triangles can be represented with one index of vertex
    except the first triangle. 
    Deering \cite{218391} first introduced generalized triangular mesh, 
    which combining the generalized triangle strips with a vertex buffer,
    which is used to further reduce the bits used to represent indices. 
    Furthermore, Chow \cite{267103} introduced a method to
    decompose a mesh into triangle strips. 
    Bajaj, Pascucci and Zhuang \cite{789628} presented a coding method with layered
    decomposition. 
    Typically, a triangle layer is a generalized triangle strip. 
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{strip.eps}
    \caption{(A)The triangle strip, (B)the triangle fan, and (C) the generated triangle strip.}\label{strip}
    \end{figure}

    Taubin and Rossignac \cite{274365} proposed a method named
    \emph{topological surgery}, which changes coding of a manifold
    mesh to coding of two trees: a vertex spanning tree and the dual of the triangles.
    Topological surgery has been implemented in MPEG-4 standard, and more details
    can be seen in the report of Taubin\cite{3d:Taubin}.
    
    Touma and Gotsman \cite{triangle:Touma} introduced a
    valence-driven approach to encode the connectivity by coding the
    valence of all the vertices in a specific traversing order. 
    The performance is further improved
    by Alliez and Desbrun \cite{alliez01valencedriven}, who also proved a
    upper-bound 3.24 bpv, exactly the theoretical value computed
    by Tutte \cite{census:Tutte} after enumerating all possible planar
    graphs.

    Several methods based on triangle conquest have also been
    proposed, such as \emph{cut-border machine} proposed by 
    Gumhold and Stra\ss{}er \cite{280836}.
    A significant advantage of this method is that it can be easily implemented
    in hardware and the decompression is very fast, which
    renders it to be very useful in real-time coding. 
    Another method named \emph{edgebreaker} is presented by Rossignac \cite{614421},
    and it is similar to the cut-border machine but has better
    compress efficiency and much slower decompression speed.
    
    After introduction of connectivity coding, we briefly introduce geometry
    coding. Most single-rate geometry compression methods involve three steps:
    quantization, prediction, and entropy coding.

    In VRML, geometry data is represented with 32 bit floating-point
    number, but this precision is highly beyond the perceiving
    capability of human eyes. Hence, quantization can be performed to
    compress the geometry data. Typically, each coordinate in a mesh is
    uniformly quantized to 8-16 bits integer.

    After quantization of vertex coordinates, prediction is taken in
    encoding by exploiting correlations between adjacent vertex
    coordinates. An effective prediction scheme generates prediction
    errors with highly compact distribution, which can be effectively
    encoded by entropy coders, such as Huffman coder or arithmetic
    coder. 

    More details about single-rate coding can be seen in the tutorial
    by Gotsman, Gumhold, and Kobbelt \cite{gotsman-simplification},
    the survey by Taubin \cite{3d:Taubin}, by Alliez and Gotsman
    \cite{recent:alliez}, and the survey by Peng, Kim and Kuo
    \cite{technologies:peng}.
    
    \subsubsection{Progressive Coding}
    Single-rate coded meshes can only be decoded after they are
    totally received, so they are not suitable for streaming. Therefore,
    some progressive coding schemes have been proposed. According to
    how the original mesh is simplified, these coding scheme can be
    categorized into three classes: progressive mesh based, vertex
    clustering based, and re-meshing based.

    In vertex clustering based schemes, space is divided into cells
    following regular structure such as kd-tree \cite{566591} and 
    octree \cite{1073237}. All vertices inside a cell is represented
    by one delegate. Cells can be further divided into smaller cells
    and the quality of reconstructed model increases. Vertex clustering
    based coding can code geometry information efficiently, but it is 
    difficult to code connectivity information. Moreover, the quality
    of base mesh is poor.

    Re-meshing based method only keeps the shape of a model
    and does not keep the accurate topology. 
    A irregular mesh can be converted into a semi-regular mesh
    with similar shape. In a semi-regular
    triangle mesh, the valence of most vertices is six, and therefore
    the connectivity information can be coded almost without cost.
    More details about these algorithms can be seen in the survey by Allienz and Gotsman
    \cite{recent:alliez}, the survey by Peng, Kim and Kuo
    \cite{technologies:peng} and the references therein.
    In addition to be not able to keep the correct topology information,
    re-meshing based method usually code a mesh as a whole, so there is 
    no flexibility in choosing sending order.

    In this thesis, we concentrate on progressive meshes, which provide 
    the finest granularity in choosing sending order, so 
    we introduce the progressive mesh based coding in more details.

    The progressive mesh (PM) \label{progressive_mesh}is first
    introduced by Hoppe in 1996 \cite{hoppe96progressive}. In this
    scheme, the simplification is based on \emph{edge collapse}, which
    collapses an edge into a vertex (see Figure \ref{split2}).
    After a series of edge collapse, a coarser \emph{base mesh} remains. 
    Then the PM is constructed by the base mesh and a series of \emph{vertex
    splits}, the inverse of edge collapse, in a reverse order.
    Rendering can be done in receiver side at any time after the base
    mesh is received. More vertex splits received, better
    approximation to the original mesh generated. After all
    the vertex split operations are received, the original mesh can be
    reconstructed without loss. \footnote{quantization error may exist
    when quantization is taken in geometry compression.}

    The base mesh can be encoded with the single-rate coding methods
    introduced in section \ref{single_rate}. Here, we introduce
    how to encode the vertex splits. A vertex split can be represented
    as $vsplit(s, l, r ,A)$, where $s$ is the index of vertex to be
    split; $l$, $r$ are the indices of two vertices with which new
    edge will be added, and $A$ represents the discrete and scalar
    attribute associated to the affected faces and corners (tuple of
    vertex and face). (see Figure \ref{split2})
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{split2.eps}
\caption{Edge collapse and vertex split}\label{split2}
\end{figure}
    This operation will add new vertex $v_{t}$, two
    edges $\{v_{t}, v_{l}\}, \{v_{t}, v_{r}\}$, two faces $\{v_{s},
    v_{t}, v_{l}\}$, $\{v_{t}, v_{s}, v_{r}\}$, and the attributes
    associated to all affected faces and corners. For connectivity
    information, we need to code $s$, $l$, and $r$, and for geometry
    information, we need to code the new position of $v_{s}$ and the
    position of $v_{r}$. If the number of vertices in current model is
    $V$, and $s$ can be any one of them, to encode $s$ need
    ${\lceil}log_{2}V{\rceil}$ bits. For $l$ and $r$, since they are
    only possible among the neighbors of $v_{s}$, and the average
    valence of a vertex is 6, in average they only have $\mathrm{P}_{2}^{6} =
    30$ choices and need $log_{2}30 \approx 5$ bits. Therefore, the total
    cost for connectivity is about ${\lceil}log_{2}V{\rceil} + 5$ bpv.
    In geometry coding, the coordinates is first quantized to 16 bit
    and then delta prediction is used. Since in PM construction, usually one
    edge is only collapsed to either the midpoint or one of the
    endpoint, this property is used in prediction coding.

\begin{comment}
    Hoppe's paper \cite{hoppe96progressive} is really important
    not only because it is the first
    paper to introduce a mesh can be progressively transmitted, but
    also because it is one solution for many problems: mesh
    simplification, various level of detail, mesh compression,
    progressive transmission and selective refinement.

    Based on Hoppe et al.'s
    \emph{mesh optimization} \cite{166119}, Hoppe presented a
    construction scheme for progressive mesh \cite{hoppe96progressive}. 
    In this scheme, not only the shape but
    also the discrete and scalar attributes (see section
    \ref{property} in page \pageref{property}) are considered. The
    candidate of edges to collapse is chosen based on its energy
    value. The energy value of a edge is decided by the effect of
    collapse to the distance error, scalar attributes error and
    discontinuity curves error. Therefore, a near optimal PM is
    constructed in a specific order of edge collapse operations. Least
    important edges are removed first in simplification and restored
    last in reconstruction, while the most important edges are removed
    last and transmitted and restored first. That means the coarse
    model in receiver side is as close to the original as possible in
    any time of streaming, if the sending order is strictly as same as
    the reverse of construction order.

    This order, however, can be changed to obtain other two
    objectives. One object is selective refinement, in another word,
    to transmit only those useful refinements, such as the vertices in
    observer's vision area. Since the index $s$ is encoded with
    ${\lceil}log_{2}V{\rceil}$ bits, it can be an arbitrary vertex in
    current model, the selective refinement is easy to be implemented
    with the feedback of receiver side. But since the vertex
    split operation also depends on $v_{l}$ and $v_{r}$, these two
    vertices should be generated in receiver side already. Therefore,
    the selectivity is not totally free. We discuss this topic in
    section \ref{sec_dependency}. 
    The other objective is compression efficiency. 
    The flexibility to choose arbitrary vertex to split 
    requires ${\lceil}log_{2}V{\rceil}$ bpv to encode $s$. 
    If we choose vertex to be split according to a specific order 
    such that correlation exists between successive vertices, 
    the entropy will decreased and hence the cost will decrease.
\end{comment}

    Hoppe \cite{efficient:hoppe} presented an efficient implementation
    of PM later. In this paper, Hoppe argued that by reordering the
    vertex split such that next one is as close to current one as
    possible, the cost of connectivity can be reduced to 10.4 bpv,
    which is $O(V)$ instead of $O(Vlog_{2}V)$ in the original paper
    \cite{hoppe96progressive}. It is worth noting that the increased
    efficiency is reached by sacrifice of optimum of the construction
    of PM, since the order has no consideration of preserve the
    quality now. In addition, the selective refinement property is
    sacrificed with the loss of flexibility. In Karni, Bogomjakov, and
    Gotsman's paper \cite{602153}, specific order of vertex split is
    also used to reduce the cost of connectivity. The
    vertex sequence is generated by recursively cut a mesh to two
    balanced part with minimum cut. With their algorithm, the connectivity can
    be encoded with in average 4.5 bpv. Moreover, their objective is
    not only the compress efficiency but also the rendering speed. In
    their paper, the criteria of choosing vertices is to maintain high
    locality and continuity of vertices so that the render process can
    be accelerated in modern graphics hardware with vertex buffer. 

    Another idea is to group many vertex splits together, 
    and efficiency will be improved at the cost of coarser granularity. 
    Taubin, Gu\'{e}ziec, Horn, and Lazarus \cite{280834} proposed the
    progressive forest split scheme (PFS), in which several edge
    collapse operations are combined together and the inverse operation splits a
    forest of edges and vertices. With one forest split, the vertex
    number will increase about $50\%$ to $100\%$; accordingly, the
    connectivity coding cost is from $10$ bpv to $7$ bpv. In addition,
    since a lot of vertex split operations are done at one time, pre
    and post smoothing can be used to decrease the geometry coding
    cost by up to $20-25\%$. Payarola and Rossignac \cite{614450},
    proposed compressed progressive mesh (CPM)\label{cpm} with similar
    idea. They group about $50\%$ vertex splits into one batch. To
    differentiate vertices to be split with others, one bit per vertex
    is used as indicative. CPM encode the connectivity with about 7
    bpv. CPM also improve the geometry coding efficiency by predicting
    the collapsed vertices position from their neighbors' coordinates.
    It is reported that the geometry coding cost is almost half as
    much as the cost of PFS.

    With the similar idea of combining decimation operations together
    but based on vertex removal (see section \ref{vertex_removal} in
    page \pageref{vertex_removal}), some progressive coding algorithms
    are proposed. In Cohen-Or et al.'s paper \cite{319358},
    vertex removals in one iteration are grouped together. In one
    iteration, only non-adjacent vertices can be chosen to remove.
    Holes will be left after vertex removals, and they should be
    re-triangulated. A set of triangles to fill in a hole is called a
    patch. Then with a 4-coloring scheme, we can identify these
    patches with 2 bits. In some specific cases, they can be
    identified even with 2-coloring which needs only 1 bit. Similar
    to CPM, the neighbors' information are used in geometry coding.
    Their experiment results show that connectivity coding cost is
    about 6 bpv and geometry coding cost is about 16-22 bpv with
    12-bit quantization resolution.

    Some single rate coding algorithms are also extended to code
    progressive meshes based on vertex removal. Bajaj, Pascucci and
    Zhuang \cite{319426} extended layered decomposition algorithm
    \cite{789628} so that it can support coding of progressive meshes
    of arbitrary topology. The connectivity cost is about 10-17 bpv
    and the geometry cost is about 30 bpv. Alliez and Desbrun
    \cite{383281} extended their valence-driven conquest
    method \cite{alliez01valencedriven}to code progressive manifold
    meshes. The efficiency of their method is quite good: on average
    3.7 bpv for connectivity coding and 10-16 bpv for geometry coding
    with 10 to 12-bit quantization resolution. For a mesh with high
    regularity and uniformity, the cost can be even much less.

    Another kind of extension of PM is to extend it to derectly encode non-manifold meshes. 
    Popovic and Hoppe \cite{258852} proposed Progressive simplicial Complexes (PSC), in
    which vertex unification and generalized vertex split are used instead of edge
    collapse and vertex split. Since these two transformations are no
    longer topology type preserved, topology type may change in the
    simplification. More bits are need, however, to encode the
    connectivity, because more vertices are involved in vertex
    unification than edge collapse. The connectivity coding cost is about $log_{2}V
    + 7$ bpv in PSC, bigger than $log_{2}V + 5$ bpv in PM. Moreover,
    the computation complexity of PSC is high. Therefore, the
    authors suggest to use a hybrid PM + PSC to reduce the cost and
    improve the efficiency since even in non-manifold meshes most areas
    are locally manifold.
    
    \subsection{3D streaming}
    \subsubsection{Error Resilient Streaming}
    Given the background in progressive mesh, we now describe
    related research in network transmissions of progressive mesh. 
    Three main classes of work exist 
    -- error resilient compression, error control, and
    packetization.

    Existing work in robust mesh compression aims to
    reduce dependencies among the mesh \cite{error:Park,error:Yan}.
    Similar to introducing key frames or restart marker in video/image
    coding, mesh
    segmentation is used to reduce the affected range of one
    packet loss. In robust mesh compression, a mesh is typically
    divided into several independent parts and then coded separately.
    Therefore the effect of one packet loss is confined to the part to which
    it belongs. The finer the partition is, the fewer the affected vertices
    are.  The coding efficiency, however, will decrease
    due to more redundancies and less correlation.

    Al-Regib and Altunbasak \cite{unequal:Al-Regib} proposed an
    unequal error protection method to improve the resilience of
    progressive 3D mesh based on CPM (compressed progressive meshes). Forward error
    correction (FEC) codes are added to the
    base mesh and additional levels-of-detail information to maximize 
    the decoded mesh quality.  The method is similar to FEC protection 
    of video data.
    As argued in the previous section, we believe that for streaming
    of 3D meshes, retransmission is always a better choice (except in
    cases such as multicast where retransmission is not scalable).
    Chen et al. \cite{chen05hybrid} also applied FEC to streaming
    progressive meshes. They analyzed several transmission schemes:
    TCP only, UDP only, TCP with UDP, and UDP with FEC, and studied
    their effects experimentally on the transmission time and decoded
    mesh quality.
    Al-Regib et al. proposed an application layer protocol, 3TP,
    for streaming of 3D models \cite{3tpregib}, combining both TCP 
    and UDP.  In 3TP, important packets are sent using
    TCP, while the rest are sent with UDP to minimize delay.
    
    \subsubsection{Packetization}
    Packetization of different model is tackled by
    Harris III and Karvets in \cite{harris:design}.   They proposed
    a protocol named On-Demand Graphic Transport Protocol (OGP)
    for transmitting 3D models represented as a tree of bounding volumes.
    A key component of the protocol is to decide which bounding volumes
    to send.  OGP begins with packing the largest possible subtree at
    the root and continues to pack the nodes in the subtree of
    acknowledged nodes in breadth-first order.  
    Gu and Ooi \cite{Gu:Packetization} were the first to look at
    the packetization problem for progressive meshes.  They model
    the packetization problem as a graph problem where the objective
    is to equally partition the graph into $k$ partitions with minimum
    cut size.  The problem is shown to be NP-complete and a heuristic
    is proposed.  They, however, assume that every vertex split is
    equally important.  In practice, the importance of vertex splits can
    vary considerably.  
    \subsubsection{View-Dependent Streaming}
    The view-dependent approach first appeared as 
    a dynamic simplification method used for adaptive rendering of a complex 3D mesh
    \cite{258843, 258847}. Only vertex splits that contribute to the rendered
    image will be rendered, allowing real-time rendering of a complex mesh
    even with limited rendering capability.
    Besides progressive mesh, other multi-resolution representations, 
    such as vertex-clustering  and subdivision scheme,
    are used in view-dependent refinement systems \cite{245627, efficient:Alliez,602344}.

    Later, the view-dependent approach is used in progressive 
	streaming of 3D meshes.     In the scheme proposed by Southern et al. \cite{363375},  the client is stateless and
    maintains only the visible data. 
    To et al. \cite{To1999}
    and Kim et al. \cite{kim:view} proposed that received data are stored
    in the receiver even after they become invisible, 
    so they need not be resent when they are visible again. 
    In these papers, view-dependent approaches mainly aim at addressing
    limited rendering capability. 
    
    Yang et al. \cite{progressive:Yang} and
     Zheng et al. \cite{zheng:interactive}, on the other hand, use
     view-dependent streaming to address limited network bandwidth.
     Yang et al. proposed a scheme where the server chooses the appropriate resolution
     according to the available network bandwidth.
     Zheng et al. \cite{zheng:interactive} use prediction to
     reduce the effect of network latency and 
     compensate the round-trip delay with the rendering time.
     These systems use sender-driven approach and do not address
     server scalability issues.

    %\subsection{Reduce Dependency}
%\label{ss:reduce-dependency}
    The main challenge of these view-dependent schemes is 
    finding an appropriate subset of vertex splits to generate a 
    satisfactory rendered image on the client side.
    The flexibility of choosing a subset of vertices,
    hence, is crucial in view-dependent streaming. But this flexibility is
    restricted by the dependency among the vertex splits.
    For a manifold mesh, a vertex split operation depends on the existence of (a)
    the vertex to be split ($V_s$ in Figure \ref{dstream:split}), (b) two
    cut neighbors ($V_l, V_r$ in Figure \ref{dstream:split}). More dependencies exist if artificial
    folds are strictly forbidden \cite{258843, 258847}, but in this paper we
    ignore these dependencies since we can tolerate temporary folds in our scheme.

     To et al. \cite{To1999} further remove the second dependency.
     In their method, if a cut neighbor does not exist during a vertex split,
     its ancestor will be used as the cut neighbor instead.
     Kim and Lee \cite{kim01truly} improve this method so that the final mesh
     can keep the original connectivity. 
     %In their paper, when the original cut neighbor
     %is below the vertex front, its active ancestor is chosen instead. If the original
     %cut neighbor is above the vertex front, then we choose a proper active descendant of it.
     %As long as the two cut neighbors are not the same vertex, the vertex split can be processed.
     Kim et al. \cite{multiresolution:kim} propose a better scheme that enables
     an ordinary progressive mesh to be split in random order.
     This method is applied in our protocol to reduce the cut neighbor dependency
     and will be described in further details in Section \ref{s:dstream:protocol}.

     The flexibility in choosing split order, however, increases the difficulty
     in developing an effective encoding scheme. Most compression
     algorithms for progressive mesh %\cite{280834,319426, 614450, 383281,602153}
     choose a specific order of vertex splits
     to reduce redundancy by exploring the correlation
     between consecutive vertex splits.
     Moreover, compressed data can only be sequentially decoded
     so we cannot change the sending order. One solution
     proposed by Yang et al. \cite{progressive:Yang}
     is to divide the whole mesh into several segments and encode them
     separately to trade off between flexibility and compression efficiency.
     The weakness is that the size of the base mesh is relatively large
     since the original vertices in the border of segments are kept in the base
     mesh. Furthermore, the quality of the base mesh is uneven.

     Some related work \cite{multiresolution:kim, random:yoon}
     have proposed compression algorithms that allow random splitting of a mesh without
     sacrificing compression efficiency.   These algorithms are not designed for 
	 network transmission, but our scheme extended several ideas from Kim et al. \cite{multiresolution:kim}
     and applied them in view-dependent streaming.  
\begin{comment}
    \subsection{Comparison of Progressive Coding Algorithms}\label{effieciency_flexibility}
    Only coding methods that support progressively lossless transmission
    of meshes are compared here, since other methods are out of scope
    of this paper. The efficiency is the factor considered by most
    papers studying coding and compression. For view-independent
    streaming over an error-free network, the efficiency maybe the
    only factor to be considered. For streaming over a
    network with packet loss, however, the dependency between packets should be
    considered since one packet loss may affect other packets. For
    view-dependent streaming, the flexibility to choose packets to
    send also needs consideration. Therefore, in our comparison of
    algorithms, their efficiency, dependency between data, and
    flexibility in sending order are all considered.

    \subsubsection*{Efficiency}
    We give the comparison of the efficiency of the progressive
    algorithms in Table \ref{efficiency}. It is worth noting that the
    results are related to the models used in the experiments and not
    all the models used in their experiments are the same. Therefore,
    it is just a coarse comparison.

    From this table, we can see that the valence-driven algorithm
    proposed by Alliez and Desbrun \cite{383281} has the
    best efficiency in the class of progressive mesh based algorithms
    and its efficiency is comparable with the single-rate coding
    algorithms. Moreover, the algorithms based on vertex clustering
    proposed by Gandoin et al.\cite{566591} and the one proposed by
    Peng el al.\cite{1073237} are also outstanding.
\begin{table}
\begin{tabular}{|c|c|c|c|c|}
\hline
Category&Algorithm&connectivity&geometry&applicability\\
&&cost (bpv)&cost (bpv)&\\
\hline
Progressive&Hoppe\cite{hoppe96progressive}&O($Vlog_{2}V$)&-&manifold\\
meshes&Hoppe\cite{efficient:hoppe}&10.4&21-36(12)&manifold\\
&Popovic et al.\cite{258852}&O($Vlog_{2}V$)&-&arbitrary\\
&Karni et al.\cite{602153}&4.5&-&manifold\\
&Taubin et al.\cite{280834}&7-10&20-40(6)&manifold\\
&Pajarola et al.\cite{614450}&7&12-15(8,10,12)&manifold\\
&Alliez et al.\cite{383281}&3.7&10-16(10,12)&manifold\\
&Cohen-Or et al.\cite{319358}&6&16-22(12)&arbitrary\\
&Bajaj et al.\cite{319426}&10-17&30(10,12)&arbitrary\\
\hline
vertex&Gandoin et al.\cite{566591}&3.5*&15.7*(10,12)&arbitrary\\
clustering&Peng et al.\cite{1073237}&$<$\cite{566591}&$<$\cite{566591}&arbitrary\\
\hline
\end{tabular}
\caption{Comparison of the efficiency of progressive coding
    algorithms. In the column of geometry cost, the value inside the
    parenthesis represents the quantification resolution of the models
    used in the experiments. *: the result of Gandoin et
    al.\cite{566591} is for manifold meshes and results for
    non-manifold meshes are 8.2/17.1.}\label{efficiency}
\end{table}

    \subsubsection*{Dependency between data}
    The dependency can be categorized into two class: dependency
    between refinement operations and dependency caused by coding. We
    introduce the former first. For an vertex split operation for a
    manifold mesh (see figure \ref{simplification} in page
    \pageref{simplification}), the parent node $s$ and the two
    neighbor nodes $l$ and $r$ are needed. Therefore, this vertex
    split depends on the vertex split operations generating $s$, $l$
    and $r$. Hence, typically one operation depends on three
    operations. In vertex insertion, all the neighbor vertices are
    needed and hence the operation depends on the vertex insertions
    generating all the neighbor vertices. Since the average valence of
    a vertex is six, one operation in average depends on six operations.
    The dependency between the refinement operations can be
    represented as a directed acyclic graph (DAG) (see Figure \ref{dependency}(A)). 
    For point-based models such as QSplat \cite{rusinkiewicz:qsplat,364350}, 
    the dependency is simpler since no connectivity information is considered. 
    Only the dependency between parents and children exists. 
    Therefore, the dependency of QSplat can be represented as a tree (see Figure \ref{dependency}(B)).
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{dependency.eps}
\caption{Dependency: (A)DAG, (B)tree, and (C)chain. vs: vertex split or vertex insertion. r: general refinement.}\label{dependency}
\end{figure}

    The coding algorithms itself will cause dependency. In
    Hoppe\cite{efficient:hoppe} and Karni et al.\cite{602153}, the
    sequence of operations has been decided and the difference of
    geometry between two successive vertex split operations is
    encoded. Therefore, every operation depends its previous one. 
    This kind of dependency can be represented as a chain (see Figure \ref{dependency}(C)).
    In those algorithms to group a lot of operations together and
    encode them as a whole \cite{280834, 614450, 383281,
    319358, 319426}, the unit is a group of refinements instead of a
    single refinement. The dependency is still a chain, but the node
    of a chain includes many refinements. Therefore, when they are
    packed into packets, one node should be packed into many packets.
    In the vertex clustering based algorithms \cite{566591, 1073237},
    if all data are coded with arithmetic coder, then one bit error
    will cause the following bits not be able to be decoded.
    Therefore, without segmentation the dependency of these algorithms
    are also a chain.

    The dependency as a chain is not suitable for streaming in a lossy network. 
    Once a packet is lost, every following packet sent cannot be decoded 
    until the lost packet is retransmitted.
    To reduce these kind of dependency, however, may reduce efficiency. 
    For example, in Hoppe\cite{hoppe96progressive}, 
    every refinement is coded independently, 
    but ${\lceil}log_{2}V{\rceil}$ bits should be used in coding the index of the vertex split
    operation and hence the cost of connectivity is O$(V{\lceil}log_{2}V{\rceil})$. 
    How to find a balance between more efficiency and less dependency 
    becomes a topic for research. Some related papers are introduced in section \ref{segmentation}.

    \subsubsection*{Flexibility in sending order}
    In view-dependent streaming, flexibility in sending order is
    important to ensure only the needed part is transmitted. 
    The flexibility, however, depends on both the dependency
    between refinements and the granularity of refinements. 
    First, If the mesh is encoded before it is transmitted, 
    the chain dependency disables any flexibility. 
    Second, for the algorithms grouping refinements
    together, the coarse granularity also reduces the flexibility.

    A tradeoff between efficiency and flexibility also needs to be
    considered. For example, since every refinement is encoded
    independently in Hoppe\cite{hoppe96progressive}, its flexibility
    is the best during all the progressive meshes based algorithms but
    at the cost of lowest efficiency. The algorithms based on vertex
    clustering \cite{566591, 1073237} have even better flexibility
    since one operation depends on only one parent. To keep
    this kind of flexibility, however, the arithmetic coder cannot be
    applied to a large data unit since it will reduce efficiency. A
    balance between efficiency and flexibility is also a challenging problem. 
    Some papers about how to increase the flexibility are introduced in section \ref{flexibility}
    
\end{comment}
\begin{comment}
    \section{View-dependent Streaming}
    \label{s:intro:view_dependent}
    View-dependent technique is first proposed in adaptive rendering of complex 3D meshes. 
    View-dependent rendering systems onlyÂ render verticesÂ that contribute
    to the quality of rendered image. 
    The invisible vertices are not rendered. 
    Moreover, if the quality of a region of a mesh is good enough, 
    the vertex splits that further refine this region are ignored. 
    Therefore, real-time rendering of a complex mesh becomes possible 
    even for systems with limited rendering ability because the number
    of vertices to be rendered is significantly reduced.Â 

    Later, View-dependent technique is applied in progressive streaming of 3D meshes [4, 6, 7].
    SouthernÂ etÂ al. [6] propose a system in which the client is stateless 
    and only stores the visible vertices of meshes. 
    In this system, when client changes viewpoint, 
    the server will send the newly visible vertices and the client 
    will drop the vertices out of the viewable area.Â 
    When a client returns to a previous viewpoint, 
    the sender has to send again the data previously sent already 
    and the bandwidth is wasted inÂ duplicateÂ transmission. 
    Therefore,Â this system is not appropriateÂ for transmission
    over networks with limited bandwidth.Â 

    ToÂ etÂ al. [7] and KimÂ etÂ al. [4] prevent the server 
    from sending duplicate data by storing the received vertices data 
    in the client even when they become temporarily not needed.Â 
    Later, when the client returns to a previous viewpoint, 
    the data needed are still available from local storage, 
    so duplicate transmission is avoided. 
    TheseÂ methods reduce the bandwidth needed in transmission, 
    but now the server has to remember that which part of mesh is sent
    and which part is not for each client.Â 
    To keep such information for each client increases the memory usage of the server, 
    and it makes the application of caching proxy and peer-to-peer technique, 
    two common solutions to increase the scalability, hard to be used. Â 
    Moreover, their studies are mainly addressing the limited rendering ability of the client,
    which is the bottleneck in that time. 
    Now with the advance of commodity graphic card technology, 
    the rendering ability increases so much that the network bandwidth becomes the main bottleneck
    in most cases. Hence, the focus of view-dependent streaming systemsÂ needs to be changed
    to address reducing transmission bandwidth.

    YangÂ etÂ al. [8] andÂ ZhengÂ etÂ al. [10] address reducing transmission bandwidth in their research. 
    The server in the system proposed by YangÂ etÂ al. [8] decides the resolution of meshes
    according to the available network bandwidth.Â 
    ZhengÂ etÂ al. let the server predict users' viewpoints so that the server could send data in advance 
    to compensate the round-trip time with the rendering time.Â 
    In both systems, the sender decides which part of data to send based on complex algorithms. 
    When the mesh size and number of receivers are both large, the requirement of computing resources
    may beyond the ability of the server. 
    Hence, a more scalable way to support view-dependent steaming of large progressive meshes 
    to many receivers needs to be designed.
    
    In designing a view-dependent streaming system, 
    some challenges need to be considered. 
    In this section, the main challenges and the solutions in previous work are briefly introduced.

    The primary task of view-dependent streaming is finding anÂ appropriate subset of refinements
    to generate a rendered image with satisfactory quality. 
    The flexibility of choosing vertex splits to send is crucial to enable this task, 
    but it is restricted by the dependency among the vertex splits. 
    Therefore, to reduce the dependency among vertex splits becomes a challenge in view-dependent streaming. 
    Some studies have been done to address this challenge. 
    ToÂ etÂ al. [7] remove the cut-neighbor dependency by using a vertex's ancestor if it is not generated yet.Â 
    Their method is innovative butÂ cannot preserveÂ the original topology 
    and hence cannot be applied when accurate topology is required. 
    Kim and Lee [3] propose another way to remove the cut-neighbor dependency 
    and ensure the final mesh having the correct topology. 
    KimÂ etÂ al. [4] further improve this method and significantly improve the compression efficiency. 
    Â Their scheme is neat since it significantly increases the flexibility in choosing vertex splits
    and meanwhile keeps the original connectivity. 
    It is applied in this thesis to reduce the cut-neighbor dependency.

    Another Challenge is how to keep the flexibility in choosing sending order
    when progressive meshes are encoded and compressed. 
    Most compression algorithms for progressive meshes choose a specific encoding order
    to reduce theÂ redundancy by exploiting the correlation between consecutive vertex splits. 
    Moreover, the mesh compressed with traditional methods can only be sequentially decoded, 
    making changing sending order impossible. 
    Kim and Lee [2] propose a compression scheme allowing random decoding order
    and the same method is used byÂ YoonÂ andÂ LindstromÂ [9]. 
    In the same time, the efficiency of their methods is close to the best compression algorithms available.Â 
    Their studies are very important since now we have the flexibility in choosing sending order, 
    which is essential in view-dependent streaming, with little loss of compression efficiency.Â 
    Although their methods are designed for local accessing of meshes, 
    it is possible to extend them to be used in network transmission.Â 
\end{comment}
    \section{Problem Statement}
    \label{s:intro:problem}
    (summary)
    \begin{itemize}
        \item No quantitative analysis has been done for 
            finding the effect of dependency 
            when progressive meshes are transmitted over lossy networks. 
        \item Sender-driven view-dependent streaming
            is not scalable to large number of receivers. 
        \item No peer-to-peer system 
            exists for view-dependent progressive mesh streaming. 
    \end{itemize}
    
    \section{Objectives and Scope}
    \label{s:intro:objectives}
    To address the problems introduced in Section 1.5, 
    the objectives of this thesis are listed as follows: 
    \begin{enumerate}
        \item The first objective of this thesis is
            to quantitatively analyze how sending order affects
            the rendered quality of progressive meshes 
            during transmission over lossy network. 
            To achieve this objective, 
            an analytical model is proposed
            to estimate the decoding time of
            each vertex split by considering both
            the mesh property and network condition. 
            This model could help us to find some insights
            about the effect of dependency. 
            In addition, it may be applied to find a better sending strategy, 
            and a greedy strategy is given in the thesis as an example. 
            Compared with methods based on simulation, 
            analytical model is more efficient and may be used
            in the real time so that the sending order
            can be adaptive to the network condition.
        \item The second objective is to implement view-dependent streaming
            in a more scalable way. A receiver-driven approach is proposed 
            to move the visibility decision from the sender to the receiver.
            The receiver-driven approach increase the scalability
            not only because it reduce the the CPU overhead of the sender 
            but also because it makes the caching proxy and peer-to-peer much easier
            to be applied to remove the bandwidth bottleneck on the sender side.
            The application of receiver-driven approach could eventually
            make possible the view-dependent streaming of large meshes
            to huge number of receivers.
        \item The third objective of this thesis to examine the possibility 
            of implementing a peer-to-peer view-dependent streaming system. 
            The receiver-driven approach proposed in this thesis 
            makes the peer-to-peer system possible, 
            but some challenges still exist such as how to reduce the response time
            and message overhead. 
            The prototype proposed in this thesis could give some insights
            to the future research on this area. 
            Eventually, peer-to-peer system should be an attractive way 
            to bring the streaming of large meshes into practice. 
    \end{enumerate}

    Only streaming of static 3D objects is considered in this thesis. 
    Streaming of 3D animations beyonds the scope of this thesis. 
    Moreover, streaming of 3D scenes is not concerned in this thesis.
    Instead, this thesis concentrates on streaming of one large mesh.
    Nonetheless, streaming of 3D scenes becomes easier when mature methods
    of streaming of single objects exist because a 3D scene is the collection
    of multiple single meshes.Â 
    Finally, this thesis focuses on the streaming of progressive meshes. 
    Streaming of other representations of 3D objects is not discussed in this thesis,
    but the methods proposed in this thesis should be possible
    extended to support streaming of other representations with some modifications.

    In next chapter,Â existed studies about streaming of progressive mesh
    will be introduced. 
    First, various ways to improve the quality when progressive meshes are transmitted
    over lossy network are reviewed.
    Second, previous view-dependent streaming systems of progressive meshes are introduced
    and why these sender-driven approaches are not scalable is explained. 
    Finally, peer-to-peer techniques are introduced briefly
    and the difference between peer-to-peer video streaming
    and peer-to-peer progressive mesh streaming is discussed in detail.Â 
\include{mesh}
\include{dstream}
\include{mm09-3dstreaming}
\include{p2p}

\chapter{Conclusion}
\label{c:conclusion}
This research concentrated on streaming of progressive meshes over network. 
First, the effect of dependency over lossy network was quantitatively analyzed. 
To enable quantitative analysis, an analytical model was developed to compute 
the probability distribution of the decoding time of each vertex split. 
Many experiments were carried out to verify the accuracy of the analytical model, 
and different network traces and progressive meshes were used to avoid bias. 
In these experiments, the predicted decoding time of vertex splits obtained 
with the analytical model was reasonably close to the measured one. 
Therefore, with the help of the analytical model, 
the quality of the received mesh could be predicted efficiently before transmission.
Consequently, adaptive transmitting vertex splits according to the network condition
becomes possible. Besides streaming of progressive meshes, 
streaming of other partially ordered data could also be modeled
with the analytical model in this thesis. 
For example, this model was successfully applied in streaming of 3D plants.

The analytical model in this thesis assumes that the packet losses 
are independent with each other, 
so a network trace with many burst packet losses may cause considerable error in the prediction. 
Gilbert model of packet loss may be used to obtain results that are more accurate, 
but using Gilbert model may significantly increase the complexity of the analytical model. 
Moreover, burst packet losses happen less commonly in the Internet
after widespread application of RED in routers. 
Therefore, the precision of the analytical model
without considering burst packet loss may be acceptable in most cases now.

It is observed in the experiments that the effect of dependency
is only significant in the short period at the beginning of transmission, 
and the length of this period increases
when the round trip time or packet loss rate increases. 
This observation is consistent with the result of the analytical model. 
Since each lost packet will be retransmitted as soon as the packet loss is detected, 
the effect of a packet loss is restricted to a certain period. 
This effect decreases with time because of two reasons. 
First, the contribution of the vertex splits sent decreases; 
second, the quality of the reconstructed mesh increases. 
Consequently, the relative value of the effect, 
represented as the ratio of the contribution of vertex
splits to the quality of the reconstructed mesh, decreases.
As a result, considering the dependency among the vertex splits 
is only needed for applications requiring high interactivity
because these applications require low initial latency. 
For these applications, the greedy method proposed in this thesis
could generate a adaptive sending strategy to reduce the initial latency. 

View-dependent streaming is needed in streaming of large meshes
to improve the quality quickly, but current implementations cannot be easily scaled
to large number of receivers because the sender is not stateless. 
To address this weakness, a receiver-driven view-dependent streaming system,
in which the sender is stateless, was proposed in this thesis. 
According to the experimental results, the receiver-driven approach
can reduce both the CPU usage and outgoing bandwidth of the sender significantly. 
Moving the visibility decision to the receiver reduces the overhead of the server. 
Moreover, it also reduces the outgoing bandwidth of the sender because the server
does not need to send the identifications of the vertex splits any more. 
Above all, a significant advantage of receiver-driven approach is that
now both the caching proxy and P2P techniques can be applied to increase the scalability of the system
because of the sender is stateless. 

The visibility decision made by the receiver may not be accurate 
because the receiver can only estimate the visibility based on
the partially received mesh. 
Visible vertices may be not displayed if their ancestor vertices are not visible. 
According to the current results, however, this kind of error is negligible in most cases. 
For applications that tolerate no error, 
further research could be done to remove this kind of error. 
A possible solution could be to split those vertices near the silhouette even when they are invisible.

To increase the scalability of the view-dependent mesh-streaming systems, 
a P2P mesh streaming system was proposed in this thesis based on the receiver-driven approach. 
According to the experimental results, the server overhead can be reduced by 90\%
and the response time is still close to the low bound. 
The control overhead introduced in this system is about 10\%, a reasonable value. 
The response time is low because most requests in this system are successful in the first try, 
and the message overhead is reasonable because the hierarchical group system proposed in this thesis effectively 
reduces the message overhead. 
The prototype developed in this thesis shows that P2P mesh streaming system is feasible
and it can significantly reduce the server cost when there are many clients.

The control overhead at 10\% is acceptable in this system since the majority of
control messages are transmitted among the peers and the server overhead keeps low. 
Nonetheless, the message overhead could be further reduced. 

During the study, it is found that in some cases the bottleneck can be the rendering of the meshes, 
especially when the graphic card is used in deciding the visibility of vertices. 
Improving the rendering efficiency of the progressive mesh during the streaming 
could be an interesting research problem.  
During the streaming, a progressive mesh with only partial modification is frequently re-rendered, 
so it is possible to exploit the similarity between two consecutive meshes to improve the rendering speed.
\bibliographystyle{abbrv}
\bibliography{thesis}
\end{document}
