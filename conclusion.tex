\chapter{Conclusion}
\label{c:conclusion}
This thesis concentrates on streaming of high-resolution progressive meshes over the network. 
The objective is to improve the quality of rendered image in the receiver side
as quickly as possible, or in other words, to improve the quality curve of reconstructed mesh
in the receiver side. 

This thesis contributes to this objective in three aspects.
First, we quantified and minimized the negative effect of dependency
among the vertex splits when packet losses happen during transmission.
Second, we proposed a more scalable way to implement view-dependent streaming
system, which can significantly improve the rendering quality curve by considering 
each user's viewpoint. 
%With our receiver-driven protocol, view-dependent steaming system
%is easier to be adapted to a large number of users.
%Third, we try to understand the user interaction on progressively transmitted 
%high-resolution meshes, and then we can use prediction and caching to further 
%improve the quality curve. Moreover, arbitrary large number of synthetic traces 
%can be generated to further tune the system to achieve better performance.
Finally, with the help of the receiver-driven protocol we proposed, we developed 
a prototype of P2P view-dependent progressive mesh streaming system so that
a receiver can request data from many peers to improve the quality of the reconstructed mesh.

In this chapter, we briefly review the main contributions of this thesis and point
out the possible future work, including both extensions of existing work and new research problems.

\section{Contributions and Extensions}
\subsection{An Analytical Model to Quantify the Effect of Dependency and Packet Loss}
When progressive meshes are streamed over lossy networks, a packet loss affects
not only the vertex splits inside this packet, but also all the following vertex
splits with ancestors in this packet. Hence, the dependency is one of the important 
factors to be considered in deciding the sending order when progressive meshes are 
streamed over the network. In this thesis, an analytical model was developed 
to estimate the quality curves generated by various sending orders, with the dependency
among vertex splits and the network conditions as the input.
Many experiments were carried out to verify the accuracy of this analytical model, 
and different network traces and progressive meshes were used in these experiments
to avoid bias. 
According to the experimental results, 
the predicted decoding time of vertex splits using the analytical model 
was reasonably close to the measured one. 
As a result, this analytical model could be used to efficiently predict 
the quality of the received mesh before transmission. 
%Moreover, it enables
%and adaptive transmission of vertex splits according to the network condition
%becomes possible. 

In the experiments, we observed that the effect of dependency
is only significant in the short period at the beginning of the transmission, 
and the length of this period increases when the round trip time or the packet loss rate increases. 
This observation is consistent with the results deduced from the analytical model. 
Here we give an intuitive explanation. 
Each lost packet will be retransmitted
as soon as the packet loss is detected, 
so the effect of a packet loss is restricted to a certain period. 
During the transmission, the effect of lost packets decreases 
because (i) the contribution of a packet decreases;
(ii) the quality of the reconstructed mesh increases. 
Consequently, the relative value of a packet, 
represented as the ratio of its contribution
to the quality of the reconstructed mesh, decreases.
As a result, the dependency among the vertex splits 
only needs to be considered in applications requiring high interactivity,
because these applications require low initial latency. 
For these applications, a greedy method was proposed in this thesis
to generate an adaptive sending strategy to reduce the initial latency. 

Some extensions could be done to improve the analytical model introduced 
in this thesis. 
First, the precision of the model could be improved by considering bursty packet losses.
The analytical model in this thesis assumes that the packet losses 
are independent of each other, so a network trace with many bursty packet losses
may cause considerable error in the prediction,
as we already showed in the experimental results in Chapter \ref{c:model}. 
Gilbert model of packet loss may be used to obtain more accurate results, 
but using Gilbert model may significantly increase the complexity of the analytical model. 
Considering bursty packet losses happen less commonly now in the Internet after 
Random Early Detection (RED) \cite{floyd:random} was widely used in routers, 
It needs to be investigated first whether the benefit of applying Gilbert model justifies
the increasing complexity.
%Therefore, the precision of the analytical model
%without considering burst packet loss may be acceptable in most cases now.

Second, our model is general enough for any partially ordered data 
as long as it can be represented as a DAG.
For example, this model was successfully applied in streaming of 3D plants \cite{plant:seb, compact:mondet}.
Many other media types also satisfy this requirement. For example, 
in point-based representations of 3D objects, the dependency could be represented as trees, 
and a tree is a simple DAG. 
In Layered Video Streaming (e.g. H264-SVC), the dependency in one GOP is also a tree. 
Hence, the model in this thesis could also be extended to support modeling streaming of these media types.
Moreover,a tree is simpler than a general DAG in that each node has only one direct parent. 
As a result, it is possible to compute the decoding time in a recursive way, 
which is easier than the method introduced in Chapter \ref{c:model}.
If we know the distribution of the decoding time of vertex $v$, $D_v$, then the decoding time of its child $c$ can 
be represented as $min(R_c, D_v)$, where $R_c$ is the receiving time of vertex split $c$.

Third, the quality of the received mesh in the first several seconds
can be further improved by introducing FEC (Forward Error Correction) packets,
so that the packet loss may be recovered before the retransmission. % and hence reduce the response time.
Introducing FEC packets, however, is not free of costs because it will delay the sending time of following packets. 
Hence, a trade-off between sending
more FEC packets to increase the recovery probability 
and sending less FEC packets to send following vertex splits
earlier 
should be made to optimize the quality curve. 
Our model may help in finding the proper trade-off because the 
quality curve can be estimated efficiently for various number of FEC packets.

\subsection{Receiver-Driven View-Dependent Mesh Streaming Protocol}
View-dependent streaming significantly improves the quality curve
by considering each user's viewpoint.
Current implementations, however, 
cannot be easily scaled to a large number of receivers as the sender is stateful. 
To address this weakness, a receiver-driven view-dependent streaming system,
in which the sender is stateless, was proposed in this thesis. 
In this system, the visibility decision is moved from the sender side 
to the receiver side, and the server now only sends back the data on-demand.
Hence, a significant advantage of the receiver-driven approach is that
now both the caching proxy and the P2P technique can be applied
to increase the scalability of the system as the sender is stateless. 

One main challenge of the receiver-driven protocol is to enable the receiver
to determine the visibility and visual contribution of the vertex splits 
before receiving them.
In this thesis, the screen area of a vertex split was used 
to estimate its visibility and visual contribution. 
A method to detect the silhouette was also proposed and 
high priority could be assigned to the silhouette when the
profile of the mesh is important. 
Experimental results showed that these methods both work well 
in improving the quality curve. 

Further research could investigate into more sophisticated methods
to estimate the visual importance of vertex splits.
For example, we could consider the roughness, the transient level,
or the semantic meaning of different parts of the mesh in estimating
their visual importance. 


In this thesis, silhouette is simply
detected by looking for pixels being neighbor to background pixels. 
Self silhouette cannot be detected by this method. 
More sophisticated silhouette detection algorithms can be used to address this problem.
For example, the depth of each pixel or the variation of face normal values
could be used in detecting the silhouette. 
The efficiency of these methods, however, should be considered to prevent
the silhouette detection from being the bottleneck of the real-time rendering.

Finally, finding a proper objective view-dependent quality metric 
remains a challenging problem.
In our experiments, the PSNR value of rendered images was used in 
comparing the quality of reconstructed meshes. 
It sometimes did not comply well with the human perception. For example, 
an image with a block error in a small area may have the same PSNR value as an image
with error pixels scattered all over the image. 
The former, however, has worse perception because a block
error is easier to be detected and distracts the viewer's attention. 
Some other objective quality metric, such as SSIM, could be further
investigated to address this problem. 
%No satisfying objective quality metric to measure the quality
%of rendered image, which balance well between avoiding large block error and 
%keeping correct silhouette. 
%Therefore, to provide a better quality metric could be an interesting future work. 

In the receiver-driven approach proposed in this thesis, 
the visibility decision of the receiver may not be completely accurate 
since the receiver can only estimate the visibility based on
the partially received mesh. 
Visible vertices may not be displayed if its parent vertex has no visible neighbor faces.
According to the current results, however, this kind of error is negligible in most cases. 
Moreover, we found that this error can be significantly reduced by force splitting
the neighbors of vertices in the silhouette.
For applications that tolerate no error, however, 
further studies are required to completely remove this error. 
%TODO{possible solution?}

\subsection{P2P View-Dependent Progressive Mesh Streaming}
To increase the scalability of the view-dependent mesh-streaming systems, 
a P2P mesh streaming system was proposed in this thesis based on our receiver-driven approach.
Two main problems in implementing a P2P view-dependent mesh streaming system were addressed.

The first problem is how to group vertex splits into chunks. Chunking in P2P mesh streaming system
cannot follow a fixed order as the time order in P2P video streaming because of the variety in user 
interests. In this thesis, a chunking method following the vertex hierarchy is proposed and implemented. 
With this method, which chunk a vertex split belongs to can be 
implicitly known, and all vertex splits in a chunk are close to each other. 
Moreover, the dependency among chunks are minimized such that each chunk has only one direct parent. 

The second problem is how to design the content discovery system when
peers have to frequently looking for new providers in a P2P view-dependent progressive 
mesh streaming system. 
Two methods, central lookup and hierarchical P2P lookup system, are proposed
to target networks with different scales. 
To verify the effect of these two methods, 
we do simulations using
practical parameters obtained from the Internet. 
According to the simulation results, 
the server overhead can be reduced by 90\%
with the cost of slightly longer response time (less than one second)
and larger control overhead (around 10\%).
%are acceptable. %still close to the low bound. 
%The control overhead introduced in this system is about 10\%, a reasonable value. 
%The response time is low because most requests in this system are successful in the first try, 
%and the message overhead is reasonable because the hierarchical group system proposed in this thesis effectively 
%reduces the message overhead. 
%The prototype developed in this thesis shows that P2P mesh streaming system is feasible
%and it can significantly reduce the server cost when there are many clients.

The control overhead is around 10\% in a hierarchical P2P lookup system, and the larger control overhead is 
mainly caused by the leader informations exchanged among leaders and between leaders and members. 
How frequently these informations are exchanged depends on the updating interval, a preset parameter. 
Using a larger updating interval could reduce the control overhead. 
The response time, however, may increase due to more request failures caused by out-date informations. 
Hence, further research could investigate the effect of updating interval and try to find a proper
updating interval to trade off between control overhead and response time.
%reduce this overhead, we could 
%It is acceptable since the majority of
%control messages are transmitted among the peers and the server overhead is kept low. 
%Nonetheless, the control overhead could be further reduced in the future. 
%TODO{elaborate}

The response time, which is less than one second in both systems, is acceptable in many applications without 
strict real time requirement. For those application requiring strict low latency, such
as 3D games, pre-fetching can help to reduce the response time. 
To enable effective pre-fetching, the user's next action should be predicted with high accuracy.
The preliminary work we have done on studying user behaviors has already been introduced in Chapter \ref{c:user}.
The confidence level of our results, however, is greatly compromised by limited number of samples.
In the future, if we could develop a practical application, such as an online shop or an online museum,
and put it online, we may collect a large number of real traces, 
with which we could better understand the pattern of user behaviors and have a higher
confidence in predicting user actions.
%TODO{elaborate}
%It could be an interesting 
%research topic to improve the P2P system by applying pre-fetching technique. %with the insight from our user study.
%TODO{improvements}

The server overhead of the hierarchical P2P lookup system is slightly higher than the centralized lookup system, 
mainly due to the leader failures. Once a leader fails, many requests will go to the server for help. 
The server has to behave as a provider temporarily because the provider list is lost with the failed leader together. 
To overcome this problem, a backup leader, which maintains a copy of the provider list, 
could be introduced for each chunk. 
Once a leader fails, the server can immediately promote the backup leader 
and the provider list is recovered. 
As a result, the chance that the server has to be a backup provider can be greatly reduced.
%(backup leader)
%In a view-dependent progressive mesh streaming system, the interaction of users directly
%affect which data is streamed and in which order they are streamed.
%We have done some preliminary study on how users interact 
%with a progressively streamed mesh.
%With the analysis of user records, we found that user actions are 
%somewhat predictable since the user tends to repeat the previous 
%action. 
%Therefore, pre-fetching becomes possible and it can be used both in streaming and rendering to
%reduce response time.
%Furthermore, the analysis of user traces reveals that locality exists in both data and viewpoint access. 
%We can exploit this locality and use caching technique in progressive mesh streaming systems and rending systems.

%After studying the user records, 
%We proposed a simple model 
%to generate synthetic traces based on real traces we collected. 
%These synthetic traces have some similar properties with the real traces and 
%could be used in simulations to measure system performance.
%Due to the limited number of user traces collected, we have to 
%sacrifice the accuracy in this simple model. 

\section{Other Future Work}
Streaming of progressive meshes is still a relatively new area, where many interesting research problems exist.
In the last part of this thesis, in addition to the extensions mentioned above,
some selected new research problems are listed as future works.
These research problems are categorized into three groups: coding (in the sender side), 
transmission (between the sender and the receiver), and rendering (in the receiver side).
%In addition to the extensions we mentioned above, more interesting topics exist to be examined.
\subsection{Coding}
The main objective of the coding of progressive meshes is to compress vertex splits more efficiently. 
Meanwhile, the flexibility in selectively requesting vertex splits cannot be sacrificed. 
In the implementation introduced in this thesis, 
each vertex split can be separately requested even after compression. 
In some cases, such as the P2P system introduced in Chapter \ref{c:p2p}, 
where the data is organized in the chunk level, 
this kind of vertex split level granularity is not necessary. 
Therefore, it is possible to exploit the correlation between vertex splits in the same chunk to further
improve the compression efficiency. 
For example, coding the geometry data of vertex splits can be improved by 
predicting the coordinates of new vertices from the surrounding vertices and coding the error of the prediction 
with entropy coding algorithms.

Another approach to improve the geometry coding is to encode coordinates with progressive precision. 
Currently, the coordinate of a vertex has the highest precision immediately after it is generated. 
This method unnecessarily increases the data size streamed in the initial stage.
As we showed in Chapter \ref{c:rdstream}, in the initial stage, 
the geometry code of a vertex split costs more than 32 bits but it may cost as low as 16 bits later.
This cost is not desired in some applications requiring low response time in the initial stage.
To address this problem, it is possible to send coordinates with low precision first, 
and improve the precision by sending extra data later (e.g. when this vertex is to be split). 
Then the size of geometry data of a vertex split can be more even 
and the initial quality of reconstructed mesh could be improved faster.

%TODO{Modeling part: 1. other representations (geometry images?) 2. Better compression inside a chunk.
Another interesting problem is how to generate the base mesh. 
First, it is useful to know what is the optimal size of the base mesh.
The base mesh should be large enough to provide acceptable initial quality, 
but cannot be too large to avoid long downloading time.
Furthermore, how to generate the base mesh is also important. 
A carefully generated base mesh will have better silhouette and 
reduce the error in estimating visibility by the receiver.
%3. Better base mesh (ideal mesh size; better silhouette) 4. Progressive precision of the geometry data.}

This thesis concentrates on coding one single large mesh without textures.
Further studies could be done to enable coding textures and a scene with many meshes. 
The textures themselves can be compressed as images with mature algorithms. 
For example, the progressive JPEG, could be a good candidate. 
Two main problems, however, still remain to be solved.
First, how to integrate the textures with the meshes together to support progressive streaming.
Second, how to ensure the texture coordinates of each vertex to be consistent in different level
of details. 

To extend our system to a scene with multiple meshes could be implemented by adding
one more level, including mesh level visibility detection and mesh level importance determination.
I believe these directions lead to a large amount of work that themselves are enough for another Ph.D. thesis.
Here, only some preliminary thoughts are listed.
First, occlusion detection and Z-culling algorithm could be used to remove those invisible objects efficiently. 
Second, screen-area based method may also be applied to mesh level importance determination.
Usually, the objects that are far from the viewer have lower importance. At the same time, 
the faces of these far objects have smaller screen-area. Therefore, screen-area based method
already takes the distance of objects into consideration.

\subsection{Transmission}
End-to-end delay is another important factor to affect the user experience in 
a mesh streaming system.
Many research problems relate to how to reduce the end-to-end delay.
First, caching proxies that deployed close to the end users could significantly
reduce the end-to-end delay. To effectively use the resource of a caching proxy,
it is important to know which part of a mesh is most popular.
We have obtained some interesting findings from the preliminary analysis of the real user
traces introduced in Chapter \ref{c:user}. 
We found that in our limited number of traces and specific usage scenario, 
the locality exists in the data access pattern of users.
Therefore, it is possible to store a small part of the mesh and satisfy a large number
of requests. Moreover, our study shows that user actions are somewhat predictable.
Hence, pre-fetching based on predicting users' next actions can also be used to significantly
compensate the end-to-end delay.
%TODO{Streaming part: 1. pre-fetching, 2. Caching, 3. authentication}

For some applications, it is important to ensure the data is from the trusted sources without
being tampered. As a result, authentication is required if caching proxies or P2P techniques
are applied in these applications. 
To enable finding the corrupted data as soon as possible, the authentication should be
done progressively. It is non-trivial to implement the progressive authentication in mesh
streaming system because the data may be sent in many different orders 
and the sender does know the sending order in advance.
How to embed the authentication information into progressive mesh then becomes
an interesting and challenging research problem.
%
%Two kinds of data structure exists for progressive meshes: edge-based and face-based. The former is more suitable
%for 
\subsection{Rendering}
%TODO{Data structure to support decoding and rendering progressive meshes.}
%TODO{Progressive Rendering}
In some cases, we found that the rendering of meshes in the local 
computer can be the bottleneck in a mesh streaming system, 
especially when the GPU is also used to decide the visibility of vertices. 
Improving the rendering efficiency of the progressive mesh during the streaming 
could be an interesting research problem.  
During the streaming, a progressive mesh with only partial modification is frequently re-rendered, 
so it is possible to exploit the similarity between two consecutive version of reconstructed meshes
to improve the rendering speed.
For example, we could represent some of the rendered details as the textures in the next version of
the mesh to avoid redundant rendering.
Moreover, it is interesting to see whether some rendered part in the frame buffer can be 
reused in the following frames as long as the viewpoint is not changed.

%TODO{Peer assisted Rendering}
Moreover, remote rendering, in which the sender sends the rendered images directly to the receivers, 
is an effective way to enable devices without enough rendering power to 
join the mesh streaming system. An extended P2P systems, where some peers not only provide the raw vertex splits
but also provide rendered images, could be developed to further enable the 3D streaming applications on resource
constraint devices.
%\end{comment}
